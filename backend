#Back End // Fast.API & Python - Biblotech - integration

# main.py
import os
from typing import List
from fastapi import FastAPI, File, UploadFile, HTTPException
from pydantic import BaseModel
from langchain import OpenAI, VectorDBQA
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Pinecone
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import UnstructuredAPIFileLoader
from unstructured_api import UnstructuredClient
import pinecone
import boto3
from botocore.exceptions import NoCredentialsError
import uvicorn

app = FastAPI()

# Initialize Pinecone
pinecone.init(api_key=os.getenv("PINECONE_API_KEY"), environment=os.getenv("PINECONE_ENVIRONMENT"))

# Initialize embeddings
embeddings = HuggingFaceEmbeddings()

# Initialize Unstructured API client
unstructured_client = UnstructuredClient(api_key=os.getenv("UNSTRUCTURED_API_KEY"))

# Initialize S3 client
s3 = boto3.client('s3',
                  aws_access_key_id=os.getenv('AWS_ACCESS_KEY'),
                  aws_secret_access_key=os.getenv('AWS_SECRET_KEY'))

@app.post("/api/upload")
async def upload_files(files: List[UploadFile] = File(...)):
    documents = []
    for file in files:
        # Save file to S3
        try:
            s3.upload_fileobj(file.file, os.getenv('S3_BUCKET'), file.filename)
        except NoCredentialsError:
            raise HTTPException(status_code=500, detail="S3 credentials not available")

        # Use Unstructured API to process the file
        loader = UnstructuredAPIFileLoader(file.file, unstructured_client)
        documents.extend(loader.load())

    # Split documents into chunks
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    texts = text_splitter.split_documents(documents)

    # Create vector store
    vectorstore = Pinecone.from_documents(texts, embeddings, index_name="bibliotech")

    return {"message": "Files uploaded and processed successfully"}

class QueryRequest(BaseModel):
    query: str
    synthesize: bool = False
    sort_by: str = "relevance"
    citation_style: str = "apa"

@app.post("/api/query")
async def query(request: QueryRequest):
    # Create QA chain
    qa = VectorDBQA.from_chain_type(llm=OpenAI(), chain_type="stuff", vectorstore=Pinecone.from_existing_index("bibliotech", embeddings))

    # Get answer
    if request.synthesize:
        result = qa.run(request.query)
    else:
        result = qa.get_relevant_documents(request.query)

    # Sort results
    if request.sort_by == "newness":
        result.sort(key=lambda x: x.metadata['date'], reverse=True)
    elif request.sort_by == "impact_factor":
        result.sort(key=lambda x: x.metadata['impact_factor'], reverse=True)

    # Format citations (simplified, you'd need a more robust citation formatter)
    if request.citation_style == "apa":
        # Format citations in APA style
        pass
    elif request.citation_style == "mla":
        # Format citations in MLA style
        pass
    elif request.citation_style == "chicago":
        # Format citations in Chicago style
        pass

    return {"result": result}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
